{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5911b6e-aac2-4126-8a91-776b0407f369",
   "metadata": {},
   "source": [
    "# Frequently Asked Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c1dc01-bf10-4fe7-aefc-170052fcc87c",
   "metadata": {},
   "source": [
    "In this \"appendix\" to our little online book, we collect frequently asked questions about machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae419cb-bfc5-48cb-a6bd-9ce70a16abaf",
   "metadata": {},
   "source": [
    "Q: Doesn't the gradient descent get stuck in local minima?\n",
    "\n",
    "A: Yes, in principle this can happen. However, the pictures we draw are a bit misleading, with a single coordinate axis for the parameters. In truth, we are in a high-dimensional parameter space. In this space, most fixed points (where the gradient vanishes) are actually saddle points, with some directions of positive curvature and some of negative curvature. They can still slow down learning, but at least they are not local minima. Also, in many situations one observes that several training runs end up with very different parameter configurations, but only slightly different values of the loss function, performing approximately equally well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1ac69b-03ce-4692-86a2-b47fcc3e6355",
   "metadata": {},
   "source": [
    "Q: Isn't this all just curve fitting?\n",
    "\n",
    "A: In principle, yes. However, it is \"curve fitting\" in an entirely new regime, with 100s or millions of trainable parameters. This is very different from usual nonlinear curve fitting, and the behaviour therefore is often quite unexpected. For example, people have found that one can get very good performance even in regimes where the number of parameters exceeds the number of training data points, which would be a very bad regime for usual curve fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f2875e-76d4-42ed-a66e-66b744e24ef5",
   "metadata": {},
   "source": [
    "Q: Are there rules of how to pick the corect number of layers and correct number of neurons in each layer?\n",
    "\n",
    "A: There are no precise rules. However, empirical rules work for simple problems. Imagine mapping an n-dimensional input vector to an m-dimensional output vector. Then you might start with a few (2-3) hidden layers, with their neuron numbers on the order of 100 or so. This will likely already perform reasonably and then you can optimize from there. The more important message is that ML with neural networks is surprisingly robust and often does not depend very much on the detailed choices you make (unless you work on cutting edge problems where no one has managed to get good results and you explore completely new territory)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b893bbcd-d3f7-47be-acdb-c2543c89dcc1",
   "metadata": {},
   "source": [
    "Q: How important is the activation function?\n",
    "\n",
    "A: As long as you choose a nonlinear monotonic function, every choice is probably good. People have had very good success even with the simplest possible choice, a piecewise linear function (the \"relu\", rectified linear unit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ae552d-30ea-4e0c-aee0-d695d1c975c0",
   "metadata": {},
   "source": [
    "Q: How large should my training data set be?\n",
    "\n",
    "A: That's a tricky question. A good general rule of thumb for relative standard problems is that you should have 1000s of training samples. More precisely, the answer depends on the variety of data you want to process (if you have N categories of images, probably you want tens or 100s of images for each of these categories). It also depends on how much the structure of the neural network has been tailored to the type of data. Convolutional networks are better for images than fully connected networks and need less training data. On a more advanced level, for special applications, one may inject some physics knowledge into constructing a more elaborate network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec7817-9f83-45f9-9556-18049f9b328a",
   "metadata": {},
   "source": [
    "Q: OK, but I *really* do not have that much data in my experiment. What can I do?\n",
    "\n",
    "A: One option is to pretrain on e.g. simulated trainig samples. Even if they are a bit different from the experimental data, they will already help the network learn. Afterwards, train on the experimental data. Alternatively, in some cases, one can generate new fake training data from existing one (e.g. distorting / rotating images etc.), if one knows the correct output also for the fake data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0e6fa-c1db-4eff-b1f3-2f0ad6de07f2",
   "metadata": {},
   "source": [
    "(FAQ to be continued, you are welcome to post questions via the \"open issues\" link in the github button at the top of this page!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
